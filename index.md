

# 這個是Daniel的個人主頁，正在搭建中




## 個人信息
### 姓名 Daniel Yu Cho


## 研究經歷

### 綜合業務網理論與關鍵技術國家重點實驗室                      
### 2017.9 - 2020.6

### 深圳市大數據研究院                                           
### 2019.3 - 2019.12   

### Laboratory for Artificial Intelligence in Design (AiDLab)    
### 2021.11 - present





title: Daniel Cho (曹宇)


<div class="gird-containre">
<div class="grid grid--p-2">
<div class="cell cell--12 cell--md-auto" markdown="1">
I am a research scientist in AiDLab, Hong Kong.


[GoogleScholar](https://scholar.google.com.hk/citations?user=jkEWQIYAAAAJ&hl=zh-CN), [GitHub](https://github.com/DanielCho-HK)

[choyudaniel@gmail.com](mailto:choyudaniel@gmail.com)
</div>
<div class="cell cell--12 cell--md-4 " markdown="1">
![Image](./daniel.jpg){:.border.rounded}
</div>
</div>
</div>

# News


# Publications
		<div class="span8">
						<a href="http://chenweikai.github.io/publication.html">
							<strong>Deep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction from Single Images</strong>
						</a>
						<br>
						Heming Zhu, Yu Cao, Hang Jin, Weikai Chen, Dong Du, Zhangye Wang, Shuguang Cui, and Xiaoguang Han
						<br>
						<em> <b>ECCV 2020 (Oral Presentation)</b></em>
						<br>
						<em>"a large scale dataset and benchmark for real 3D garments"</em>
						<br>
						<ul class="inline middot">
							<li><em class="fa fa-file"></em> <a href="https://kv2000.github.io/2020/03/25/deepFashion3DRevisited/">project page</a></li>
							<li><em class="fa fa-file"></em> <a href="https://arxiv.org/pdf/2003.12753.pdf">paper</a></li>
							<!--<li><em class="fa fa-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> -->
							<li>
								<em class="fa fa-bars"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(fashion3d);">abstract</a>
								<pre id="fashion3d" class="invisible_text">High-fidelity clothing reconstruction is the key to achieving photorealism in a wide range of applications including human digitization, virtual try-on, etc. Recent advances in learning-based approaches have accomplished unprecedented accuracy in recovering unclothed human shape and pose from single images, thanks to the availability of powerful statistical models, e.g. SMPL [38], learned from a large number of body scans. In contrast, modeling and recovering clothed human and 3D garments remains notoriously difficult, mostly due to the lack of large-scale clothing models available for the research community. We propose to fill this gap by introducing Deep Fashion3D, the largest collection to date of 3D garment models, with the goal of establishing a novel benchmark and dataset for the evaluation of image-based garment reconstruction systems. Deep Fashion3D contains 2078 models reconstructed from real garments, which covers 10 different categories and 563 garment instances. It provides rich annotations including 3D feature lines, 3D body pose and the corresponded multi-view real images. In addition, each garment is randomly posed to enhance the variety of real clothing deformations. To demonstrate the advantage of Deep Fashion3D, we propose a novel baseline approach for single-view garment reconstruction, which leverages the merits of both mesh and implicit representations. A novel adaptable template is proposed to enable the learning of all types of clothing in a single network. Extensive experiments have been conducted on the proposed dataset to verify its significance and usefulness. We will make Deep Fashion3D publicly available upon publication</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<!-- <li><em class="fa fa-file"></em> <a href="./papers/[ECCV18]body supplementary.pdf">suppl.</a></li> -->
						</ul>
					</div>
